# -*- coding: utf-8 -*-
"""Final Object Detection YoLo V8 & SSD Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xaNHw36TzLfmn-XXCaUB5O0udmaZB_X1
"""

from google.colab import drive
drive.mount('/content/drive')

pip install ultralytics scikit-learn matplotlib seaborn

from ultralytics import YOLO

# Load the model (use YOLOv8n, YOLOv8s, YOLOv8m, etc.)
model = YOLO('yolov8n.pt')  # You can use other pre-trained variants

# Train the model
results = model.train(
    data='/content/drive/MyDrive/Obj Detection/Obj. Detection.v1/data.yaml',  # your data.yaml file path
    epochs=20,
    imgsz=640,
    batch=16,
    name='yolo-object-detector'
)

metrics = model.val()  # Evaluate the trained model
print(metrics)

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score
from sklearn.preprocessing import label_binarize
import seaborn as sns
import matplotlib.pyplot as plt
import os

y_true, y_pred = [], []

# Predict on validation images
results = model.predict(source='/content/drive/MyDrive/Obj Detection/Obj. Detection.v1/valid/images', save=False)

for result in results:
    # ðŸ”¹ Get true label from label file
    label_file = result.path.replace('/images/', '/labels/').replace('.jpg', '.txt').replace('.png', '.txt')
    if os.path.exists(label_file):
        with open(label_file, 'r') as f:
            lines = f.readlines()
            if lines:
                cls_true = int(lines[0].strip().split()[0])  # first object only
                y_true.append(cls_true)

                # ðŸ”¹ Get prediction
                if result.boxes and len(result.boxes.data) > 0:
                    cls_pred = int(result.boxes.data[0][5].item())  # top-1 prediction
                    y_pred.append(cls_pred)
                else:
                    y_pred.append(-1)  # no detection

print("âœ… Accuracy:", accuracy_score(y_true, y_pred))
print("\nâœ… Classification Report:\n", classification_report(y_true, y_pred))

cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

# Convert to one-hot for ROC-AUC (only valid if >1 class)
classes = sorted(set(y_true + y_pred))
y_true_bin = label_binarize(y_true, classes=classes)
y_pred_bin = label_binarize(y_pred, classes=classes)

fpr, tpr, _ = roc_curve(y_true_bin.ravel(), y_pred_bin.ravel())
auc_score = roc_auc_score(y_true_bin, y_pred_bin, average='macro')

plt.plot(fpr, tpr, label=f"ROC curve (AUC = {auc_score:.2f})")
plt.plot([0,1], [0,1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC-AUC Curve")
plt.legend()
plt.grid()
plt.show()

results = model.predict(source='/content/drive/MyDrive/Obj Detection/Obj. Detection.v1/test/images/3_moinul_jpg.rf.2469368869182cd571f6eb2b70ab4154.jpg', save=True, conf=0.5)

import os
import torch
import torch.nn.functional as F
import numpy as np
from PIL import Image
import cv2
from torchvision.transforms.functional import to_tensor
from ultralytics import YOLO
import yaml
import matplotlib.pyplot as plt

# -------- Config --------
weights_path = '/content/runs/detect/yolo-object-detector/weights/best.pt'  # Your trained weights path
img_path = '/content/drive/MyDrive/Obj Detection/Obj. Detection.v1/test/images/3_moinul_jpg.rf.2469368869182cd571f6eb2b70ab4154.jpg'
data_yaml_path = '/content/drive/MyDrive/Obj Detection/Obj. Detection.v1/data.yaml'  # Dataset yaml file
imgsz = 640
out_dir = '/content/drive/MyDrive/Obj Detection/Figures/gradcam_all_objects_fixed'
os.makedirs(out_dir, exist_ok=True)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
torch.backends.cudnn.benchmark = True

# -------- Helpers --------
def normalize_cam(cam):
    cam = cam - cam.min()
    if cam.max() > 0:
        cam = cam / cam.max()
    return cam

def overlay_heatmap_on_image(img_np, cam, alpha=0.5):
    heatmap = (cam * 255).astype(np.uint8)
    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)
    overlay = (1 - alpha) * (img_np.astype(np.float32) / 255.0) + alpha * (heatmap.astype(np.float32) / 255.0)
    overlay = np.clip(overlay, 0, 1)
    return (overlay * 255).astype(np.uint8)

# -------- Load class names from data.yaml --------
with open(data_yaml_path, 'r') as f:
    data_yaml = yaml.safe_load(f)
class_names = data_yaml.get('names', None)
if class_names is None:
    raise RuntimeError("Could not find 'names' field in data.yaml")

# -------- Load model and predict --------
model = YOLO(weights_path)
model.fuse()
model.model.to(device)

results = model.predict(source=img_path, conf=0.25, save=False)
if len(results) == 0 or len(results[0].boxes) == 0:
    raise RuntimeError("No detections found")

res0 = results[0]

# -------- Image preprocessing --------
orig_pil = Image.open(img_path).convert('RGB')
orig_np = np.array(orig_pil)
orig_h, orig_w = orig_np.shape[:2]

img_resized = orig_pil.resize((imgsz, imgsz))
img_tensor = to_tensor(img_resized).unsqueeze(0).float().to(device)
img_tensor.requires_grad_(True)

# -------- Select target conv layer --------
target_layer = None
try:
    target_layer = model.model.model[-2]
except Exception:
    for m in reversed(list(model.model.model.modules())):
        if isinstance(m, torch.nn.Conv2d):
            target_layer = m
            break
if target_layer is None:
    raise RuntimeError("Couldn't find a conv layer to hook into.")

# -------- Hooks --------
saved_acts = {}
saved_grads = {}

def forward_hook(module, inp, out):
    saved_acts['value'] = out  # keep for grads

def backward_hook(module, grad_in, grad_out):
    saved_grads['value'] = grad_out[0]

if hasattr(target_layer, 'register_full_backward_hook'):
    fh = target_layer.register_forward_hook(forward_hook)
    bh = target_layer.register_full_backward_hook(lambda m, gi, go: backward_hook(m, gi, go))
else:
    fh = target_layer.register_forward_hook(forward_hook)
    bh = target_layer.register_backward_hook(lambda m, gi, go: backward_hook(m, gi, go))

# -------- Forward raw outputs --------
model.model.train()
torch.set_grad_enabled(True)
model.model.zero_grad()

raw_out = model.model(img_tensor)

def raw_outputs_to_rows_no_det(raw):
    rows = []
    if isinstance(raw, (list, tuple)):
        for r in raw:
            if r.ndim == 4:
                b, c, h, w = r.shape
                rr = r.permute(0,2,3,1).reshape(-1, c)
                rows.append(rr)
            elif r.ndim == 3:
                rr = r.reshape(-1, r.shape[-1])
                rows.append(rr)
            elif r.ndim == 2:
                rows.append(r)
            else:
                raise RuntimeError(f"Unsupported tensor shape: {r.shape}")
        return torch.cat(rows, dim=0)
    else:
        r = raw
        if r.ndim == 4:
            b, c, h, w = r.shape
            return r.permute(0,2,3,1).reshape(-1, c)
        elif r.ndim == 3:
            return r.reshape(-1, r.shape[-1])
        elif r.ndim == 2:
            return r
        else:
            raise RuntimeError(f"Unsupported raw output shape: {r.shape}")

raw_rows = raw_outputs_to_rows_no_det(raw_out).to(device)

D = raw_rows.shape[1]
if D <= 5:
    print("Warning: raw_rows has <=5 cols; class logits may be absent.")

if D > 5:
    obj_col = 4
    class_offset = 5
    objectness = raw_rows[:, obj_col]
else:
    obj_col = None
    class_offset = 5
    objectness = torch.ones(raw_rows.shape[0], device=device)

# -------- Loop over detections --------
for i in range(len(res0.boxes)):
    pred_box = res0.boxes.xyxy[i].cpu().numpy().astype(int)
    pred_score = float(res0.boxes.conf[i].cpu().numpy())
    pred_class = int(res0.boxes.cls[i].cpu().numpy())
    class_name = class_names[pred_class] if pred_class < len(class_names) else f"class_{pred_class}"
    print(f"Explaining detection {i}: class={class_name}, score={pred_score:.3f}, box={pred_box.tolist()}")

    if D > class_offset:
        class_scores_for_target = raw_rows[:, class_offset + pred_class]
        combined = objectness * class_scores_for_target
        top_idx = torch.argmax(combined).item()
    else:
        top_idx = torch.argmax(objectness).item()

    model.model.zero_grad()
    torch.set_grad_enabled(True)

    if D > class_offset:
        target_scalar = class_scores_for_target[top_idx]
    else:
        target_scalar = objectness[top_idx]

    if not target_scalar.requires_grad:
        target_scalar = raw_rows[top_idx, (class_offset + pred_class) if D > class_offset else obj_col]

    target_scalar.backward(retain_graph=True)

    if 'value' not in saved_acts or 'value' not in saved_grads:
        print(f"Missing activations or gradients for detection {i}")
        continue

    acts = saved_acts['value']
    grads = saved_grads['value']

    if acts.dim() == 4:
        acts = acts[0]
        grads = grads[0]
    else:
        print("Unexpected activation dims:", acts.shape)
        continue

    weights = torch.mean(grads.view(grads.shape[0], -1), dim=1)
    cam = torch.zeros(acts.shape[1:], dtype=torch.float32, device=device)
    for k, w in enumerate(weights):
        cam += w * acts[k, :, :]

    cam = F.relu(cam)
    cam = cam.unsqueeze(0).unsqueeze(0)
    cam = F.interpolate(cam, size=(imgsz, imgsz), mode='bilinear', align_corners=False)
    cam = cam.squeeze().cpu().detach().numpy()  # detach before numpy
    cam = normalize_cam(cam)

    cam_up = cv2.resize((cam * 255).astype(np.uint8), (orig_w, orig_h), interpolation=cv2.INTER_LINEAR) / 255.0

    x1, y1, x2, y2 = pred_box
    mask = np.zeros_like(cam_up)
    pad = max(2, int(0.02 * max(orig_w, orig_h)))
    x1c = max(0, x1 - pad); y1c = max(0, y1 - pad)
    x2c = min(orig_w, x2 + pad); y2c = min(orig_h, y2 + pad)
    mask[y1c:y2c, x1c:x2c] = 1.0
    cam_masked = cam_up * mask
    cam_final = cam_masked if cam_masked.sum() > 0 else cam_up
    cam_final = normalize_cam(cam_final)

    overlay = overlay_heatmap_on_image(orig_np, cam_final, alpha=0.6)

    label = f"{class_name}: {pred_score:.2f}"
    cv2.rectangle(overlay, (x1, y1), (x2, y2), (0,255,0), 2)
    cv2.putText(overlay, label, (max(0,x1), max(0,y1-8)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)

    # Save overlay
    out_path = os.path.join(out_dir, f'gradcam_det_{i}_cls{pred_class}_score{int(pred_score*100)}.jpg')
    cv2.imwrite(out_path, cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))
    print("Saved:", out_path)

    # Display inline in Colab
    img_show = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)  # OpenCV uses BGR
    img_show = cv2.cvtColor(img_show, cv2.COLOR_BGR2RGB)  # Convert to RGB for matplotlib
    plt.figure(figsize=(8, 8))
    plt.imshow(img_show)
    plt.title(f'Grad-CAM for {class_name} (score: {pred_score:.2f})')
    plt.axis('off')
    plt.show()

    model.model.zero_grad()
    saved_grads.clear()

# Remove hooks
fh.remove()
bh.remove()

print("Done. All Grad-CAM images saved and displayed.")

!pip install ultralytics opencv-python-headless matplotlib

import cv2
import torch
from ultralytics import YOLO
import numpy as np
from google.colab.patches import cv2_imshow
import yaml
from google.colab.output import eval_js
from base64 import b64decode

# Load your trained model weights and class names
weights_path = '/content/runs/detect/yolo-object-detector/weights/best.pt'  # your model weights path
data_yaml_path = '/content/drive/MyDrive/Obj Detection/Obj. Detection.v1/data.yaml'  # your data.yaml path

model = YOLO(weights_path)

with open(data_yaml_path, 'r') as f:
    data = yaml.safe_load(f)
class_names = data['names']
print(f"Loaded classes: {class_names}")

def preprocess_frame(frame, img_size=640):
    # Resize with letterbox padding
    h0, w0 = frame.shape[:2]
    r = img_size / max(h0, w0)  # scale ratio
    new_unpad = (int(w0 * r), int(h0 * r))
    frame_resized = cv2.resize(frame, new_unpad, interpolation=cv2.INTER_LINEAR)

    dw = img_size - new_unpad[0]
    dh = img_size - new_unpad[1]
    top, bottom = dh // 2, dh - dh // 2
    left, right = dw // 2, dw - dw // 2

    frame_padded = cv2.copyMakeBorder(frame_resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114,114,114))

    return frame_padded, r, left, top

def scale_coords_box(box, r, left, top, orig_shape):
    x1, y1, x2, y2 = box
    x1 = max((x1 - left) / r, 0)
    y1 = max((y1 - top) / r, 0)
    x2 = min((x2 - left) / r, orig_shape[1])
    y2 = min((y2 - top) / r, orig_shape[0])
    return int(x1), int(y1), int(x2), int(y2)

def capture_webcam_image():
    js = """
    async function capture() {
      const div = document.createElement('div');
      const video = document.createElement('video');
      const canvas = document.createElement('canvas');
      const ctx = canvas.getContext('2d');
      const stream = await navigator.mediaDevices.getUserMedia({video: true});
      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      ctx.drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', 0.8);
    }
    capture();
    """
    data = eval_js(js)
    header, encoded = data.split(',', 1)
    img_bytes = b64decode(encoded)
    img_array = np.frombuffer(img_bytes, np.uint8)
    img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
    return img

print("Capture an image from webcam. Please allow camera access.")
frame = capture_webcam_image()

img_size = 640
img_pre, ratio, pad_x, pad_y = preprocess_frame(frame, img_size)

results = model(img_pre, imgsz=img_size, conf=0.1)  # Lower confidence threshold

print(f"Detections found: {len(results[0].boxes)}")
if len(results[0].boxes) == 0:
    print("No objects detected! Try adjusting conf threshold or check camera input.")
else:
    for box in results[0].boxes:
        xyxy = box.xyxy.cpu().numpy()[0]
        conf = float(box.conf.cpu().numpy())
        cls = int(box.cls.cpu().numpy())
        cls_name = class_names[cls] if cls < len(class_names) else f'class_{cls}'
        print(f"Detected {cls_name} with confidence {conf:.3f}")

        x1, y1, x2, y2 = scale_coords_box(xyxy, ratio, pad_x, pad_y, frame.shape)
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)
        label = f"{cls_name} {conf:.2f}"
        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)

cv2_imshow(frame)

"""#SSD"""

!pip install torch torchvision matplotlib seaborn scikit-learn --quiet

import torch
from torch.utils.data import Dataset, DataLoader
import torchvision
from torchvision.models.detection import ssd300_vgg16
from torchvision.models.detection.ssd import SSDClassificationHead
import torchvision.transforms as T
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score
from sklearn.preprocessing import label_binarize

# --- Dataset class for YOLO format labels ---
class YoloDetectionDataset(Dataset):
    def __init__(self, img_dir, label_dir, transforms=None):
        self.img_dir = img_dir
        self.label_dir = label_dir
        self.img_files = sorted([f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png'))])
        self.transforms = transforms

    def __len__(self):
        return len(self.img_files)

    def __getitem__(self, idx):
        img_name = self.img_files[idx]
        img_path = os.path.join(self.img_dir, img_name)
        label_path = os.path.join(self.label_dir, img_name.rsplit('.',1)[0] + '.txt')

        img = torchvision.io.read_image(img_path).float() / 255.0  # [C,H,W] float tensor in [0,1]

        boxes = []
        labels = []

        if os.path.exists(label_path):
            with open(label_path) as f:
                for line in f:
                    parts = line.strip().split()
                    cls = int(parts[0])
                    x_center, y_center, w, h = map(float, parts[1:5])

                    # Convert normalized YOLO bbox to xmin,ymin,xmax,ymax absolute coords
                    img_h, img_w = img.shape[1], img.shape[2]
                    xmin = (x_center - w/2) * img_w
                    ymin = (y_center - h/2) * img_h
                    xmax = (x_center + w/2) * img_w
                    ymax = (y_center + h/2) * img_h
                    boxes.append([xmin, ymin, xmax, ymax])
                    labels.append(cls + 1)  # background=0, classes start from 1

        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0,4), dtype=torch.float32)
        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,), dtype=torch.int64)

        target = {}
        target['boxes'] = boxes
        target['labels'] = labels

        if self.transforms:
            img = self.transforms(img)

        return img, target

# --- Paths to your dataset ---
train_img_dir = '/content/drive/MyDrive/Obj Detection/Obj. Detection.v1/train/images'
train_label_dir = '/content/drive/MyDrive/Obj Detection/Obj. Detection.v1/train/labels'
val_img_dir = '/content/drive/MyDrive/Obj Detection/Obj. Detection.v1/valid/images'
val_label_dir = '/content/drive/MyDrive/Obj Detection/Obj. Detection.v1/valid/labels'

# --- Dataset and dataloader ---
train_dataset = YoloDetectionDataset(train_img_dir, train_label_dir)
val_dataset = YoloDetectionDataset(val_img_dir, val_label_dir)

def collate_fn(batch):
    return tuple(zip(*batch))

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)

# --- Load SSD model ---
num_classes = 1 + 14  # background + your 14 classes

# Channels and anchors per feature map from default SSD300 VGG16 model
in_channels = [512, 1024, 512, 256, 256, 256]
num_anchors = [4, 6, 6, 6, 4, 4]

model = ssd300_vgg16(pretrained=True)
model.head.classification_head = SSDClassificationHead(in_channels, num_anchors, num_classes)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# --- Optimizer ---
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# --- Training loop ---
num_epochs = 10
model.train()

for epoch in range(num_epochs):
    total_loss = 0
    for imgs, targets in train_loader:
        imgs = list(img.to(device) for img in imgs)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        loss_dict = model(imgs, targets)
        losses = sum(loss for loss in loss_dict.values())

        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        total_loss += losses.item()

    print(f"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss / len(train_loader):.4f}")

# --- Validation loop ---
model.eval()
y_true = []
y_pred = []

with torch.no_grad():
    for imgs, targets in val_loader:
        imgs = list(img.to(device) for img in imgs)
        outputs = model(imgs)

        for output, target in zip(outputs, targets):
            gt_labels = target['labels'].cpu().numpy()
            if len(output['boxes']) == 0:
                # no detections
                y_true.extend(gt_labels)
                y_pred.extend([-1]*len(gt_labels))
            else:
                # take predicted label of highest confidence
                pred_scores = output['scores'].cpu().numpy()
                pred_labels = output['labels'].cpu().numpy()
                top_idx = pred_scores.argmax()
                y_pred.append(pred_labels[top_idx])
                if len(gt_labels) > 0:
                    y_true.append(gt_labels[0])
                else:
                    y_true.append(-1)

# --- Metrics ---
print("Accuracy:", accuracy_score(y_true, y_pred))
print("\nClassification Report:\n", classification_report(y_true, y_pred))

cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

# --- ROC AUC ---
classes = sorted(set(y_true + y_pred))
y_true_bin = label_binarize(y_true, classes=classes)
y_pred_bin = label_binarize(y_pred, classes=classes)

fpr, tpr, _ = roc_curve(y_true_bin.ravel(), y_pred_bin.ravel())
auc_score = roc_auc_score(y_true_bin, y_pred_bin, average='macro')

plt.plot(fpr, tpr, label=f"ROC curve (AUC = {auc_score:.2f})")
plt.plot([0,1], [0,1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC-AUC Curve")
plt.legend()
plt.grid()
plt.show()

# --- Test on a single image ---
test_img_path = '/content/drive/MyDrive/Obj Detection/Obj. Detection.v1/test/images/3_moinul_jpg.rf.2469368869182cd571f6eb2b70ab4154.jpg'
test_img = torchvision.io.read_image(test_img_path).float() / 255.0
test_img = test_img.to(device)

model.eval()
with torch.no_grad():
    output = model([test_img])[0]

print("Detections on test image:")
for box, score, label in zip(output['boxes'], output['scores'], output['labels']):
    if score > 0.5:
        print(f"Label: {label.item()}, Confidence: {score.item():.3f}, Box: {box.cpu().numpy()}")